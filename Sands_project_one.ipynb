{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sands_project one.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/t4toast/project-one/blob/main/Sands_project_one.ipynb",
      "authorship_tag": "ABX9TyMAVgxA7Kr890WhIlwoVfOm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t4toast/project-one/blob/main/Sands_project_one.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project one, final\n",
        "- Diane T Sands\n",
        "\n",
        "-steps to remember:\n",
        "1. import all libraries\n",
        "2. Load the dataset\n",
        "3. make copy of df\n",
        "5. check about dataset info\n",
        "6. df.isna().sum().sum()\n",
        "7. check for duplicates and drop the duplicates - df.duplicated().sum()\n",
        "8. check for incosistacneis\n",
        "9. Split the dataset-train/test\n",
        "10. column selection - separate the categorical and numerical one data\n",
        "11. handling missing values using imputer for categorical and numerical dataset\n",
        "12. justify the strategy used in code like why are you using mean, median or mode.\n",
        "13. Make pipleines\n",
        "14. use column transformers for scaling , ohe, for the categorical and numerical data.\n",
        "15.fit the data to train and transform train/test both\n"
      ],
      "metadata": {
        "id": "N0kvLmhatR7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import make_column_selector, make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn import set_config\n",
        "set_config(display='diagram')"
      ],
      "metadata": {
        "id": "Qc6NGJ5Dy5U4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset\n",
        "filename = '/content/drive/MyDrive/Colab Notebooks/sales_predictions.csv'\n",
        "df = pd.read_csv(filename)"
      ],
      "metadata": {
        "id": "Axy5XuVmzG0c"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make a copy of the data\n",
        "ml_df = df.copy()"
      ],
      "metadata": {
        "id": "BIvpD_ShubhF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explore the data some \n",
        "ml_df. info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eetX6Qi9uOzw",
        "outputId": "8f55ca2d-ec00-4023-836b-43e6a9e895c7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8523 entries, 0 to 8522\n",
            "Data columns (total 12 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   Item_Identifier            8523 non-null   object \n",
            " 1   Item_Weight                7060 non-null   float64\n",
            " 2   Item_Fat_Content           8523 non-null   object \n",
            " 3   Item_Visibility            8523 non-null   float64\n",
            " 4   Item_Type                  8523 non-null   object \n",
            " 5   Item_MRP                   8523 non-null   float64\n",
            " 6   Outlet_Identifier          8523 non-null   object \n",
            " 7   Outlet_Establishment_Year  8523 non-null   int64  \n",
            " 8   Outlet_Size                6113 non-null   object \n",
            " 9   Outlet_Location_Type       8523 non-null   object \n",
            " 10  Outlet_Type                8523 non-null   object \n",
            " 11  Item_Outlet_Sales          8523 non-null   float64\n",
            "dtypes: float64(4), int64(1), object(7)\n",
            "memory usage: 799.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check for missing data\n",
        "ml_df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_pEddCgu0Rt",
        "outputId": "b60bb748-9721-49b5-9751-89f608074f02"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Item_Identifier                 0\n",
              "Item_Weight                  1463\n",
              "Item_Fat_Content                0\n",
              "Item_Visibility                 0\n",
              "Item_Type                       0\n",
              "Item_MRP                        0\n",
              "Outlet_Identifier               0\n",
              "Outlet_Establishment_Year       0\n",
              "Outlet_Size                  2410\n",
              "Outlet_Location_Type            0\n",
              "Outlet_Type                     0\n",
              "Item_Outlet_Sales               0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check for duplicates\n",
        "ml_df.duplicated().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwIWiKqUux0v",
        "outputId": "40b51cff-afef-4432-f9d4-230c6a775b7a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#no duplicated rows. Can drop columns Item Identifier & Outlet Year \n",
        "#Identify the features (X) and target (y): Assign the \"Item_Outlet_Sales\" column as your target \n",
        "#and the rest of the relevant variables as your features matrix.  \n",
        "X = ml_df.drop(['Item_Identifier', 'Outlet_Establishment_Year', 'Item_Outlet_Sales'], axis=1, inplace=True)\n",
        "y = ml_df['Item_Outlet_Sales']"
      ],
      "metadata": {
        "id": "VTELY4RqvIZB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "506077a3-de06-4558-d3bb-ec2c7300d663"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Item_Outlet_Sales'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-c404ead0c96d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#and the rest of the relevant variables as your features matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Item_Identifier'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Outlet_Establishment_Year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Item_Outlet_Sales'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Item_Outlet_Sales'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Item_Outlet_Sales'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform a train test split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "zPvQWq_Kv-rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make sure your imputation of missing values occurs  after the train test split using SimpleImputer.  \n",
        "#Item Weight and Outlet Size are the two categories with missing data.\n",
        "#look again at the data...\n",
        "\n",
        "ml_df['Item_Weight'].describe()  "
      ],
      "metadata": {
        "id": "QSwx0Da1wyZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml_df['Outlet_Size'].value_counts()"
      ],
      "metadata": {
        "id": "SEKh1cN20jZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a ColumnTransformer to preprocess the data. \n",
        "cat_selector = make_column_selector(dtype_include= 'object')\n",
        "num_selector = make_column_selector(dtype_include= 'number')\n",
        "scaler = StandardScaler()\n",
        "ohe = OneHotEncoder(sparse=False, handle_unknown= 'drop')\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "most_frequent_imputer = SimpleImputer(strategy='most_frequent')\n",
        "#using mean for Item Weight as it is very close to 50% of the data\n",
        "\n",
        "#make tuples\n",
        "num_selector = make_column_selector(dtype_include='number')\n",
        "cat_selector = make_column_selector(dtype_include='object')\n",
        "\n",
        "num_tuple = (scaler, num_selector)\n",
        "cat_tuple = (ohe, cat_selector)\n",
        "\n",
        "col_transformer = make_column_transformer(num_tuple, cat_tuple, remainder = 'passthrough')\n",
        "col_transformer.fit(X_train)\n",
        "X_train_processed = col_transformer.transform(X_train)\n",
        "X_test_processed = col_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "pkFTtkfIwpYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make pipelines\n",
        "number_pipe = make_pipeline(mean_imputer, scaler)\n",
        "number_pipe\n"
      ],
      "metadata": {
        "id": "rWU4Y3Aq6m5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_pipe = make_pipeline(most_frequent, ohe)\n",
        "categorical_pipe"
      ],
      "metadata": {
        "id": "_9207GUH6-ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessor\n",
        "preprocessor = make_column_transformer(categorical_tuple,\n",
        "                                       number_tuple,\n",
        "                                       remainder='drop')"
      ],
      "metadata": {
        "id": "CXK61lR7BlQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the MAE, MSE, RMSE and R2 series of code to use later\n",
        "def evaluate_regression(y_true, y_pred):\n",
        "  \n",
        "  mae = mean_absolute_error(y_true, y_pred)\n",
        "  mse = mean_squared_error(y_true, y_pred)\n",
        "  rmse = np.sqrt(mse)\n",
        "  r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "  print(f'scores: \\nMAE: {mae:,.2f} \\nMSE: {mse:,.2f} \\nRMSE: {rmse:,.2f} \\nR2: {r2:.2f}')\n"
      ],
      "metadata": {
        "id": "LrrMbbUlxmxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#linear regression model\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "lin_reg_pipe = make_pipeline(preprocessor, lin_reg)\n",
        "\n",
        "lin_reg_pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "KFznU29t1t0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run evaluation parameters\n",
        "print('Training')\n",
        "evaluate_regression(y_train, dec_tree_pipe.predict(X_train))\n",
        "print('Testing')\n",
        "evaluate_regression(y_test, dec_tree_pipe.predict(X_test))"
      ],
      "metadata": {
        "id": "NEDfomctCSx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#do a decision tree\n",
        "dec_tree = DecisionTreeRegressor(random_state = 42)\n",
        "dec_tree.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "TWFRA3VHCW12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run evaluation parameters\n",
        "print('Training')\n",
        "evaluate_regression(y_train, dec_tree_pipe.predict(X_train))\n",
        "print('Testing')\n",
        "evaluate_regression(y_test, dec_tree_pipe.predict(X_test))"
      ],
      "metadata": {
        "id": "ndTccSg6CuJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have have tried 2 different models on the data set. I need to determine which model to implement.\n",
        "\n",
        "Overall, which model do you recommend? >>> neither yet since neither of them worked.\n",
        "Justify your recommendation."
      ],
      "metadata": {
        "id": "jqU1t0TiC6FS"
      }
    }
  ]
}